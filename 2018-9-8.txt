IDEA :
   全精度网络指导low-bits网络进行裁剪，加速？
   通过比较网络不同阶段输出差异？
PAPERS :
   Towards Effective Low-bitwidth Convolutional Neural Networks
   本文解决了使用低精度权重和低位宽激活训练深度卷积神经网络的问题。优化低精度网络非常具有挑战性，因为训练过程很容易陷入较差的局部最小值，从而导致实质性的精度损失。为了缓解这个问题，我们提出了三种简单而有效的方法来改进网络培训。首先，我们建议使用两阶段优化策略逐步找到良好的局部最小值。具体而言，我们建议首先用量化权重优化网络，然后量化激活。这与同时优化它们的传统方法形成对比。其次，遵循第一种方法的类似精神，我们提出了另一种渐进式优化方法，该方法在训练过程中逐渐减小从高精度到低精度的位宽。第三，我们采用一种新颖的学习方案，与低精度模型一起共同训练全精度模型。通过这样做，全精度模型提供了指导低精度模型训练的提示。对各种数据集（即CIFAR-100和ImageNet）的广泛实验表明了所提出方法的有效性。要突出显示，使用我们的方法训练4位精度网络与使用标准网络架构的全精度对应物（即AlexNet和ResNet-50）相比，性能不会下降
