IDEA :
   全精度网络指导low-bits网络进行裁剪，加速？
   例如分类网络，反卷积网络得到输入图，将反卷积得到的输入图作为需要优化网络输入，以需要优化网络输出loss或与全精度网络输出差异作为优化判定标准。
   通过比较网络不同阶段输出差异？
PAPERS :
   Towards Effective Low-bitwidth Convolutional Neural Networks
   本文解决了使用低精度权重和低位宽激活训练深度卷积神经网络的问题。优化低精度网络非常具有挑战性，因为训练过程很容易陷入较差的局部最小值，从而导致实质性的精度损失。为了缓解这个问题，我们提出了三种简单而有效的方法来改进网络培训。首先，我们建议使用两阶段优化策略逐步找到良好的局部最小值。具体而言，我们建议首先用量化权重优化网络，然后量化激活。这与同时优化它们的传统方法形成对比。其次，遵循第一种方法的类似精神，我们提出了另一种渐进式优化方法，该方法在训练过程中逐渐减小从高精度到低精度的位宽。第三，我们采用一种新颖的学习方案，与低精度模型一起共同训练全精度模型。通过这样做，全精度模型提供了指导低精度模型训练的提示。对各种数据集（即CIFAR-100和ImageNet）的广泛实验表明了所提出方法的有效性。要突出显示，使用我们的方法训练4位精度网络与使用标准网络架构的全精度对应物（即AlexNet和ResNet-50）相比，性能不会下降.
   本文提出的解决方案包含三个组成部分。 它们可以独立使用或联合使用。 第一种方法是采用两阶段培训过程。 在第一阶段，仅量化网络的权重。 在获得第一阶段的足够好的解决方案之后，进一步要求网络的激活具有低精度并且将再次训练网络。 本质上，这种渐进方法首先解决了相关的子问题，即仅用低比特权重训练网络，并且子问题的解决方案为训练我们的目标问题提供了良好的初始点。遵循类似的想法，我们通过对网络的位宽方面执行渐进式训练来提出我们的第二种方法。 具体来说，我们逐步训练一系列网络，其量化位宽（精度）从全精度逐渐降低到目标精度。
