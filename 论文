https://arxiv.org/abs/1902.04422

Joint Training of Neural Network Ensembles

We examine the practice of joint training for neural network ensembles, in which a multi-branch architecture is trained via single loss. 
This approach has recently gained traction, with claims of greater accuracy per parameter along with increased parallelism. 
We introduce a family of novel loss functions generalizing multiple previously proposed approaches, with which we study theoretical 
and empirical properties of joint training. These losses interpolate smoothly between independent and joint training of predictors,
demonstrating that joint training has several disadvantages not observed in prior work. However, with appropriate regularization via
our proposed loss, the method shows new promise in resource limited scenarios and fault-tolerant systems, e.g., IoT and edge devices.
Finally, we discuss how these results may have implications for general multi-branch architectures such as ResNeXt and Inception. 


https://arxiv.org/abs/1902.04103

Bag of Freebies for Training Object Detection Neural Networks

Comparing with enormous research achievements targeting better image classification models, efforts applied to object detector training 
are dwarfed in terms of popularity and universality. Due to significantly more complex network structures and optimization targets,
various training strategies and pipelines are specifically designed for certain detection algorithms and no other. In this work,
we explore universal tweaks that help boosting the performance of state-of-the-art object detection models to a new level without
sacrificing inference speed. Our experiments indicate that these freebies can be as much as 5% absolute precision increase that 
everyone should consider applying to object detection training to a certain degree. 

https://arxiv.org/abs/1902.04186
Riemannian joint dimensionality reduction and dictionary learning on symmetric positive definite manifold


https://arxiv.org/pdf/1902.04378.pdf
